{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANNAlgorithms.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOrC+7VEc0KGU/WeqE8XPYl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Karishma-Kuria/CMPE-255-Data-Mining/blob/main/ANNAlgorithms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zInHnJvQHiLA"
      },
      "source": [
        "#  Approximate nearest neighbor search using various state of art library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWYhf_cviawq"
      },
      "source": [
        "## Installing all necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgHjmaEDHX-w",
        "outputId": "a8db8440-7c52-4430-b72a-3adb8830e4a2"
      },
      "source": [
        "!pip install lightfm\n",
        "!pip install nmslib\n",
        "!pip install faiss\n",
        "!pip install annoy\n",
        "!pip install faiss-cpu --no-cache"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lightfm in /usr/local/lib/python3.7/dist-packages (1.16)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from lightfm) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from lightfm) (2.23.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from lightfm) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from lightfm) (1.0.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->lightfm) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->lightfm) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->lightfm) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->lightfm) (2021.10.8)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->lightfm) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->lightfm) (1.1.0)\n",
            "Requirement already satisfied: nmslib in /usr/local/lib/python3.7/dist-packages (2.1.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from nmslib) (5.4.8)\n",
            "Requirement already satisfied: numpy>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from nmslib) (1.19.5)\n",
            "Requirement already satisfied: pybind11<2.6.2 in /usr/local/lib/python3.7/dist-packages (from nmslib) (2.6.1)\n",
            "Requirement already satisfied: faiss in /usr/local/lib/python3.7/dist-packages (1.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from faiss) (1.19.5)\n",
            "Requirement already satisfied: annoy in /usr/local/lib/python3.7/dist-packages (1.17.0)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.7/dist-packages (1.7.1.post2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAiZjCpdjNMZ"
      },
      "source": [
        "## For getting the data here I am installing Stack API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8y_l1hcEUuo",
        "outputId": "3314a024-f816-48db-a37f-fb87e2721a81"
      },
      "source": [
        "!pip install stackapi"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: stackapi in /usr/local/lib/python3.7/dist-packages (0.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from stackapi) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from stackapi) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->stackapi) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->stackapi) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->stackapi) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->stackapi) (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxMdTgPHjaMn"
      },
      "source": [
        "## **Importing all relevant libraries for the process**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADtxVuInOpwI"
      },
      "source": [
        "import pickle\n",
        "from lightfm import LightFM\n",
        "from lightfm.datasets import fetch_stackexchange\n",
        "import nmslib\n",
        "import faiss\n",
        "import annoy\n",
        "import time\n",
        "from stackapi import StackAPI"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMJ6ViRnjrIg"
      },
      "source": [
        "## Loading Data from the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hO9i1gUwEf26",
        "outputId": "7ea1d445-8ec1-4cf9-8ea3-a0b177291926"
      },
      "source": [
        "stackexchange_data = fetch_stackexchange('crossvalidated')\n",
        "stackexchange_data"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'item_feature_labels': array(['question_id:0', 'question_id:1', 'question_id:2', ...,\n",
              "        'question_id:72357', 'question_id:72358', 'question_id:72359'],\n",
              "       dtype='<U17'),\n",
              " 'item_features': <72360x72360 sparse matrix of type '<class 'numpy.float32'>'\n",
              " \twith 72360 stored elements in Compressed Sparse Row format>,\n",
              " 'test': <2836x72360 sparse matrix of type '<class 'numpy.float32'>'\n",
              " \twith 8020 stored elements in COOrdinate format>,\n",
              " 'train': <2836x72360 sparse matrix of type '<class 'numpy.float32'>'\n",
              " \twith 51477 stored elements in COOrdinate format>}"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTZbTbBCkthf"
      },
      "source": [
        "## Dividing the data into Test and Train data.\n",
        "## Model Building and training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lITzdVxIdb79"
      },
      "source": [
        "train_data = stackexchange_data['train']\n",
        "test_data = stackexchange_data['test']\n",
        "\n",
        "# defining lightfm model by specifying hyper-parametre\n",
        "# then using the ineteractions matrix to fit the model, item and user features \n",
        "model = LightFM(learning_rate=0.05, loss='warp', no_components=64, item_alpha=0.001)\n",
        "model.fit_partial(train_data, item_features=stackexchange_data['item_features'], epochs=20 )\n",
        "\n",
        "data_item_vectors = stackexchange_data['item_features'] * model.item_embeddings"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZG07RCz8djLU",
        "outputId": "0954fd65-45e2-4c9a-946f-017440dfd5a4"
      },
      "source": [
        "stackexchange_data['item_feature_labels']"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['question_id:0', 'question_id:1', 'question_id:2', ...,\n",
              "       'question_id:72357', 'question_id:72358', 'question_id:72359'],\n",
              "      dtype='<U17')"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAx_Xv9Tmm0b"
      },
      "source": [
        "## Using pickle for dumping data with item vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYvgyt43dmcS"
      },
      "source": [
        "with open('stack_exchange.pickle', 'wb') as f:\n",
        "    pickle.dump({\"name\": stackexchange_data['item_feature_labels'], \"vector\": item_vectors}, f)"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEr6LLzNmyFg"
      },
      "source": [
        "## Loading data from the above file and Visualizing it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdIzHw-FdoG3",
        "outputId": "92f974fc-8234-471d-9d27-bd8f386c49c6"
      },
      "source": [
        "def load_data():\n",
        "    with open('stack_exchange.pickle', 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "    return data\n",
        "\n",
        "dataset = load_data()\n",
        "dataset"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': array(['question_id:0', 'question_id:1', 'question_id:2', ...,\n",
              "        'question_id:72357', 'question_id:72358', 'question_id:72359'],\n",
              "       dtype='<U17'),\n",
              " 'vector': array([[ 5.5895172e-02,  1.1990369e-01, -1.1908560e-01, ...,\n",
              "          3.5561640e-02,  8.8606728e-03,  6.2211487e-02],\n",
              "        [-7.5620688e-02,  3.4251142e-02, -7.7318780e-02, ...,\n",
              "          9.4727725e-02, -5.9194420e-02,  5.2315751e-03],\n",
              "        [-8.0306500e-02,  8.2449630e-02, -1.7578368e-01, ...,\n",
              "          5.1416919e-02,  7.3899886e-05,  5.2313991e-03],\n",
              "        ...,\n",
              "        [-7.5524454e-03,  2.1180427e-03,  8.4224651e-03, ...,\n",
              "         -4.0007071e-04,  2.3719552e-03, -3.7159508e-03],\n",
              "        [ 1.2772135e-02, -3.4847087e-03,  2.0979075e-02, ...,\n",
              "          2.3611756e-02,  2.4236694e-02, -3.7828584e-03],\n",
              "        [-2.4579108e-02, -7.9067359e-03,  2.5011292e-02, ...,\n",
              "         -5.2589890e-02,  3.6753878e-02,  3.7756007e-02]], dtype=float32)}"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgT0LEK8dt52"
      },
      "source": [
        "# **LSH**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFDWSMfBzOrP"
      },
      "source": [
        "Locality Sensitive Hashing (LSH): this algorithm works by first grouping all vectors into small buckets and then processing each vector through a hash function that maximizes hashing collisions, rather than minimizing which is usual with most of the hashing functions.\n",
        "It works on the principle that if two points are closer to each other in a given space then they are supposed to have the same hash hence this puts both of them into the same bucket.\n",
        "In LSH wide range of the performance depends on parameter set. Fast search results in bad quality results while slower search results in good quality.\n",
        "This Search process is very simple to implement and is good for embedded system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEL6YnJKnVb1"
      },
      "source": [
        "## Vector Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRtlORAOduK_"
      },
      "source": [
        "item_vectors_value = dataset[\"vector\"]\n",
        "item_names_value = dataset[\"name\"]"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYZFFOIWd1ay",
        "outputId": "21aa385e-c311-4b95-aa03-d43525537bf3"
      },
      "source": [
        "item_vectors_value"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 5.5895172e-02,  1.1990369e-01, -1.1908560e-01, ...,\n",
              "         3.5561640e-02,  8.8606728e-03,  6.2211487e-02],\n",
              "       [-7.5620688e-02,  3.4251142e-02, -7.7318780e-02, ...,\n",
              "         9.4727725e-02, -5.9194420e-02,  5.2315751e-03],\n",
              "       [-8.0306500e-02,  8.2449630e-02, -1.7578368e-01, ...,\n",
              "         5.1416919e-02,  7.3899886e-05,  5.2313991e-03],\n",
              "       ...,\n",
              "       [-7.5524454e-03,  2.1180427e-03,  8.4224651e-03, ...,\n",
              "        -4.0007071e-04,  2.3719552e-03, -3.7159508e-03],\n",
              "       [ 1.2772135e-02, -3.4847087e-03,  2.0979075e-02, ...,\n",
              "         2.3611756e-02,  2.4236694e-02, -3.7828584e-03],\n",
              "       [-2.4579108e-02, -7.9067359e-03,  2.5011292e-02, ...,\n",
              "        -5.2589890e-02,  3.6753878e-02,  3.7756007e-02]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANzoYOz7npnQ",
        "outputId": "ec6e057b-80bc-4a19-b6a3-de7b265876fe"
      },
      "source": [
        "# visualizing the name values \n",
        "item_names_value"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['question_id:0', 'question_id:1', 'question_id:2', ...,\n",
              "       'question_id:72357', 'question_id:72358', 'question_id:72359'],\n",
              "      dtype='<U17')"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wZplisboZi_"
      },
      "source": [
        "## Creating LSH Index Class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4XRZRJjqoMf"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOED9GnOojDE"
      },
      "source": [
        "class Index_LSH():\n",
        "  def __init__(self, vectors, labels):\n",
        "      self.dimension = vectors.shape[1]\n",
        "      self.vectors = vectors.astype('float32')\n",
        "      self.labels = labels\n",
        "  def build_index(self, num_bits=8):\n",
        "      self.index = faiss.IndexLSH(self.dimension, num_bits)\n",
        "      self.index.add(self.vectors)\n",
        "  def results(self, vectors, k=10):\n",
        "      distances, indices = self.index.search(vectors, k) \n",
        "      return [self.labels[i] for i in indices[0]]"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4RcWeJfo_Vz"
      },
      "source": [
        "## Here I am checking the time taken by LSH to find the similar questions to question with id 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbgZ63NWo_mb",
        "outputId": "bd3a812e-82e4-445e-e0b3-cbe06ae8b736"
      },
      "source": [
        "time_taken = time.time()\n",
        "\n",
        "lsh_index = Index_LSH(data[\"vector\"], data[\"name\"])\n",
        "lsh_index.build_index()\n",
        "\n",
        "vector, name = data['vector'][5:6], data['name'][5]\n",
        "\n",
        "simlar_question_return_ids = '\\n '.join(lsh_index.results(vector))\n",
        "print(f\"Similar questions to question with id: {name} are:\\n {simlar_question_return_ids}\")\n",
        "\n",
        "time_2 = time.time()\n",
        "\n",
        "total_time = time_2-time_taken\n",
        "\n",
        "print(\"LSH took {total_time} seconds.\".format(total_time=total_time))"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similar questions to question with id: question_id:5 are:\n",
            " question_id:139\n",
            " question_id:5\n",
            " question_id:152\n",
            " question_id:132\n",
            " question_id:6\n",
            " question_id:189\n",
            " question_id:203\n",
            " question_id:207\n",
            " question_id:138\n",
            " question_id:23\n",
            "LSH took 0.022244691848754883 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nZMvbM1qgHt"
      },
      "source": [
        "# **Exhaustive Search**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1UhDn0M1BYs"
      },
      "source": [
        "Exhaustive Search is a very generic problem solving technique in which all possible element are systematically enumerated for the solution and also checked whether each element satisfies the problem's statement. It requires Linear query time which depends on the size of the dataset and the dimentions. Hence it is very tedious and time taking process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWFuoV5ZqpL0"
      },
      "source": [
        "## Creating Exhaustive Index Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMF5Pq9fqsdI"
      },
      "source": [
        "class Index_ExhaustiveSearch():\n",
        "  def __init__(self, vectors, labels):\n",
        "    self.dimension = vectors.shape[1]\n",
        "    self.vectors = vectors.astype('float32')\n",
        "    self.labels = labels\n",
        "\n",
        "  def build_index(self):\n",
        "    self.index = faiss.IndexFlatL2(self.dimension,)\n",
        "    self.index.add(self.vectors)\n",
        "\n",
        "  def results(self, vectors, k=10):\n",
        "    distances, indices = self.index.search(vectors, k) \n",
        "    return [self.labels[i] for i in indices[0]]"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sn-WyhOlq_18"
      },
      "source": [
        "## Here I am checking the time taken by Exhaustive Search to find the similar questions to question with id 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6WVpXLCrE4S",
        "outputId": "53b406b8-33dd-485b-f681-d2f58d331229"
      },
      "source": [
        "time_taken = time.time()\n",
        "\n",
        "exi_ind = Index_ExhaustiveSearch(data[\"vector\"], data[\"name\"])\n",
        "exi_ind.build_index()\n",
        "\n",
        "vector, name = data['vector'][5:6], data['name'][5]\n",
        "\n",
        "simlar_question_return_ids = '\\n '.join(exi_ind.results(vector))\n",
        "print(f\"Similar questions to question with id: {name} are:\\n {simlar_question_return_ids}\")\n",
        "\n",
        "time_2 = time.time()\n",
        "\n",
        "total_time = time_2-time_taken\n",
        "\n",
        "print(\"Exhaustive Search took {total_time} seconds.\".format(total_time=total_time))"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similar questions to question with id: question_id:5 are:\n",
            " question_id:5\n",
            " question_id:262\n",
            " question_id:889\n",
            " question_id:20491\n",
            " question_id:836\n",
            " question_id:472\n",
            " question_id:151\n",
            " question_id:372\n",
            " question_id:698\n",
            " question_id:6287\n",
            "Exhaustive Search took 0.035036325454711914 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INo39RLisVHh"
      },
      "source": [
        "# **Trees and Graphs**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHGGsnrnsYib"
      },
      "source": [
        "## Creating Tree And Graphs Index Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BO_qxwdjsXA2"
      },
      "source": [
        "class Index_Annoy():\n",
        "    def __init__(self, vectors, labels):\n",
        "        self.dimention = vectors.shape[1]\n",
        "        self.vectors = vectors.astype('float32')\n",
        "        self.labels = labels\n",
        "\n",
        "\n",
        "    def build(self, number_of_trees=5):\n",
        "        self.index = annoy.AnnoyIndex(self.dimention)\n",
        "        for i, vec in enumerate(self.vectors):\n",
        "            self.index.add_item(i, vec.tolist())\n",
        "        self.index.build(number_of_trees)\n",
        "        \n",
        "    def results(self, vector, k=10):\n",
        "        indices = self.index.get_nns_by_vector(vector.tolist(), k)\n",
        "        return [self.labels[i] for i in indices]\n"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSa9UQV8srqL"
      },
      "source": [
        "## Here I am checking the time taken by Tree and Graph Algorithm to find the similar questions to question with id 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEZhjHwNstkT",
        "outputId": "a051792f-c0a1-43f6-9ade-29473dd526d8"
      },
      "source": [
        "time_taken = time.time()\n",
        "\n",
        "annoy_ind = Index_Annoy(data[\"vector\"], data[\"name\"])\n",
        "annoy_ind.build()\n",
        "\n",
        "vector, name = data['vector'][5], data['name'][5]\n",
        "\n",
        "simlar_question_return_ids = '\\n '.join(annoy_ind.results(vector))\n",
        "print(f\"Similar questions to question with id: {name} are:\\n {simlar_question_return_ids}\")\n",
        "\n",
        "time_2 = time.time()\n",
        "\n",
        "total_time = time_2-time_taken\n",
        "\n",
        "print(\"Trees and Graphs algorithm took {total_time} seconds.\".format(total_time=total_time))"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: FutureWarning: The default argument for metric will be removed in future version of Annoy. Please pass metric='angular' explicitly.\n",
            "  if __name__ == '__main__':\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similar questions to question with id: question_id:5 are:\n",
            " question_id:5\n",
            " question_id:262\n",
            " question_id:431\n",
            " question_id:2737\n",
            " question_id:8984\n",
            " question_id:378\n",
            " question_id:88\n",
            " question_id:585\n",
            " question_id:121\n",
            " question_id:275\n",
            "Trees and Graphs algorithm took 0.8027896881103516 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01hXkYpTt6nu"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0svjjDxEtcHE"
      },
      "source": [
        "# **Product Quantization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1MpDdtHtjkY"
      },
      "source": [
        "Product Quantization is a type of Vector quantization process. It's used to increase the speed of Approximate nearest neighbour search.\n",
        "Quantization is basically used to considerably reduce or compress data which is very important task when comparing large number of arrays of vectors as they all must be loaded in memory in order to be compared. \n",
        "Compared to other quantization methods Product Quantization reduces the memory size effectively.\n",
        "Key idea behind Product Quantization is:\n",
        "*   First divide all the vectors into m subvectors called blocks.\n",
        "*   Then Vector Quantizes each subvector independently.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUy3749atkDD"
      },
      "source": [
        "### Creating Index Class for Product Quantization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54FNIASvttqg"
      },
      "source": [
        "class Index_PQ():\n",
        "    def __init__(self, vectors, labels):\n",
        "        self.dimension = vectors.shape[1]\n",
        "        self.vectors = vectors.astype('float32')\n",
        "        self.labels = labels    \n",
        "        \n",
        "    def build(self, \n",
        "              number_of_partition=8, \n",
        "              search_in_x_partitions=2, \n",
        "              subvector_size=8):\n",
        "        quantizer = faiss.IndexFlatL2(self.dimension)\n",
        "        self.index = faiss.IndexIVFPQ(quantizer, \n",
        "                                      self.dimension, \n",
        "                                      number_of_partition, \n",
        "                                      search_in_x_partitions, \n",
        "                                      subvector_size)\n",
        "        self.index.train(self.vectors)\n",
        "        self.index.add(self.vectors)\n",
        "        \n",
        "    def results(self, vectors, k=10):\n",
        "        distances, indices = self.index.search(vectors, k) \n",
        "        return [self.labels[i] for i in indices[0]]"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "179GZSY_t9jJ"
      },
      "source": [
        "## Here I am checking the time taken by Product Quantization to find the similar questions to question with id 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBNliqO-uC6C",
        "outputId": "25ab5c0f-cd71-49af-86ab-e9c7f7c6de3c"
      },
      "source": [
        "time_taken = time.time()\n",
        "\n",
        "annoy_ind = Index_PQ(data[\"vector\"], data[\"name\"])\n",
        "annoy_ind.build()\n",
        "\n",
        "vector, name = data['vector'][5:6], data['name'][5]\n",
        "\n",
        "simlar_question_return_ids = '\\n '.join(annoy_ind.results(vector))\n",
        "print(f\"Similar questions to question with id: {name} are:\\n {simlar_question_return_ids}\")\n",
        "\n",
        "time_2 = time.time()\n",
        "\n",
        "total_time = time_2-time_taken\n",
        "\n",
        "print(\"Product Quantization took {total_time} seconds.\".format(total_time=total_time))"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similar questions to question with id: question_id:5 are:\n",
            " question_id:5\n",
            " question_id:50\n",
            " question_id:67\n",
            " question_id:2\n",
            " question_id:96\n",
            " question_id:102\n",
            " question_id:114\n",
            " question_id:131\n",
            " question_id:90\n",
            " question_id:46\n",
            "Product Quantization took 3.5436084270477295 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKK8KKequYC6"
      },
      "source": [
        "# **HNSW**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxkZ_55GudOm"
      },
      "source": [
        "HNSW stands for Hierarchical Navigable Small World Graphs: These graphs are more recently developed in serach.HNSW based ANNS consistently tops as the highest performing indexes[1]. It has great search-quality, good search-speed, but substantial index sizes. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJTAAbkQucrL"
      },
      "source": [
        "## Creating Index Class for HNSW"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGBxLf4_ubdN"
      },
      "source": [
        "class Index_HNSW():\n",
        "  def __init__(self, vectors, labels):\n",
        "    self.dimention = vectors.shape[1]\n",
        "    self.vectors = vectors.astype('float32')\n",
        "    self.labels = labels\n",
        "\n",
        "  def build(self):\n",
        "    self.index = nmslib.init(method='hnsw', space='cosinesimil')\n",
        "    self.index.addDataPointBatch(self.vectors)\n",
        "    self.index.createIndex({'post': 2})\n",
        "        \n",
        "  def results(self, vector, k=10):\n",
        "    indices = self.index.knnQuery(vector, k=k)\n",
        "    return [self.labels[i] for i in indices[0]]"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbAe31COu6ie"
      },
      "source": [
        "## Here I am checking the time taken by HNSW Algorithm to find the similar questions to question with id 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pFC8LIYu_jI",
        "outputId": "13988e6b-0fb5-47a0-f4a8-bb665cea8c3c"
      },
      "source": [
        "time_taken = time.time()\n",
        "\n",
        "hnsw_ind = Index_HNSW(data[\"vector\"], data[\"name\"])\n",
        "hnsw_ind.build()\n",
        "\n",
        "vector, name = data['vector'][5:6], data['name'][5]\n",
        "\n",
        "simlar_question_return_ids = '\\n '.join(hnsw_ind.results(vector))\n",
        "print(f\"Similar questions to question with id: {name} are:\\n {simlar_question_return_ids}\")\n",
        "\n",
        "time_2 = time.time()\n",
        "\n",
        "total_time = time_2-time_taken\n",
        "\n",
        "print(\"HNSW Algorithm took {total_time} seconds.\".format(total_time=total_time))"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similar questions to question with id: question_id:5 are:\n",
            " question_id:5\n",
            " question_id:262\n",
            " question_id:47462\n",
            " question_id:889\n",
            " question_id:836\n",
            " question_id:2\n",
            " question_id:20491\n",
            " question_id:472\n",
            " question_id:372\n",
            " question_id:151\n",
            "HNSW Algorithm took 54.92410588264465 seconds.\n"
          ]
        }
      ]
    }
  ]
}